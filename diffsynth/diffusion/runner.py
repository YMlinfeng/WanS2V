import os, torch
from tqdm import tqdm
from accelerate import Accelerator
from .training_module import DiffusionTrainingModule
from .logger import ModelLogger
import torch
from tqdm import tqdm
import time
import time
from contextlib import contextmanager
from collections import defaultdict

import time
from collections import defaultdict
from contextlib import contextmanager
import statistics

class StepTimer:
    def __init__(self):
        self.times = defaultdict(list)
        self.step_keys = [] # Áî®‰∫éËÆ∞ÂΩïÈîÆÁöÑÈ°∫Â∫èÔºå‰øùËØÅÊâìÂç∞È°∫Â∫è‰∏ÄËá¥

    @contextmanager
    def time_step(self, name):
        if name not in self.step_keys:
            self.step_keys.append(name)
        start = time.perf_counter()
        yield
        elapsed = time.perf_counter() - start
        self.times[name].append(elapsed)

    def record(self, name, elapsed):
        """Áî®‰∫éÊâãÂä®ËÆ∞ÂΩïÊó∂Èó¥Ôºà‰æãÂ¶ÇÊï∞ÊçÆÂä†ËΩΩÔºâ"""
        if name not in self.step_keys:
            self.step_keys.append(name)
        self.times[name].append(elapsed)

    def print_summary(self):
        # ÂÅáËÆæÊâÄÊúâkeyËÆ∞ÂΩïÁöÑÊ≠•Êï∞‰∏ÄËá¥ÔºåÂèñÁ¨¨‰∏Ä‰∏™keyÁöÑÈïøÂ∫¶
        if not self.step_keys:
            print("Ê≤°ÊúâËÆ∞ÂΩïÂà∞‰ªª‰ΩïÊó∂Èó¥Êï∞ÊçÆ„ÄÇ")
            return
            
        num_steps = len(self.times[self.step_keys[0]])
        
        # --- ÊâìÂç∞ÈÄêÊ≠•ËØ¶ÊÉÖË°®Ê†º ---
        print("\n" + "="*100)
        print(f"{'Step ËÄóÊó∂ËØ¶ÊÉÖ (Âçï‰Ωç: ms)':^100s}")
        print("="*100)
        
        # Ë°®Â§¥
        headers = ["Step"] + self.step_keys + ["Total"]
        # Âä®ÊÄÅË∞ÉÊï¥ÂàóÂÆΩ
        col_width = 12 
        header_str = "".join([f"{h:>{col_width}s}" for h in headers])
        print(header_str)
        print("-" * len(header_str))

        # ÂÜÖÂÆπË°å
        for i in range(num_steps):
            row_vals = []
            step_total = 0.0
            for key in self.step_keys:
                val = self.times[key][i] * 1000 # ËΩ¨Êç¢‰∏∫ÊØ´Áßí
                step_total += val
                row_vals.append(f"{val:{col_width}.2f}")
            
            row_str = f"{i:>{col_width}d}" + "".join(row_vals) + f"{step_total:{col_width}.2f}"
            print(row_str)

        # --- ÊâìÂç∞ÂéüÊù•ÁöÑÂπ≥ÂùáÂÄºÁªüËÆ° ---
        print("\n" + "="*100)
        print(f"{'ÁªüËÆ°ÊëòË¶Å':^100s}")
        print("="*100)
        print(f"{'Èò∂ÊÆµ':<20s} | {'Âπ≥Âùá(ms)':>10s} | {'ÊÄªËÆ°(s)':>10s} | {'Âç†ÊØî':>8s}")
        print("-" * 60)
        
        total_time_all_steps = sum(sum(v) for v in self.times.values())
        
        for name in self.step_keys:
            values = self.times[name]
            avg = statistics.mean(values) * 1000
            total = sum(values)
            ratio = (total / total_time_all_steps) * 100
            print(f"{name:<20s} | {avg:10.2f} | {total:10.2f} | {ratio:7.1f}%")
        print("="*100)

def diagnose_default_training_status(model):
    """
    ËØäÊñ≠Ê®°ÂûãÂΩìÂâçÁöÑÈªòËÆ§ËÆ≠ÁªÉÁä∂ÊÄÅÔºàÂú®‰∫∫Â∑•‰øÆÊîπ requires_grad ‰πãÂâçÔºâ
    """
    print("\n" + "="*50)
    print("üïµÔ∏è [ËØäÊñ≠Ê®°Âºè] Ê£ÄÊü•Ê®°ÂûãÈªòËÆ§ËÆ≠ÁªÉÁä∂ÊÄÅ...")
    print("="*50)
    
    trainable_params = []
    frozen_params = []
    
    trainable_numel = 0
    frozen_numel = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            trainable_params.append(name)
            trainable_numel += param.numel()
        else:
            frozen_params.append(name)
            frozen_numel += param.numel()
            
    # ÁªüËÆ°Êï∞ÊçÆ
    total_layers = len(trainable_params) + len(frozen_params)
    total_params = trainable_numel + frozen_numel
    
    print(f"üìä ÁªüËÆ°ÁªìÊûú:")
    print(f"   - ÊÄªÂ±ÇÊï∞ (Keys): {total_layers}")
    print(f"   - ÊÄªÂèÇÊï∞Èáè (Elements): {total_params / 1e9:.2f} B (ÂçÅ‰∫ø)")
    print(f"   -------------------------------------------")
    print(f"   üîì ÂèØËÆ≠ÁªÉÂ±ÇÊï∞ (Trainable): {len(trainable_params)}")
    print(f"      - ÂèÇÊï∞Èáè: {trainable_numel / 1e9:.2f} B")
    print(f"      - Âç†ÊØî: {trainable_numel / total_params * 100:.2f}%")
    print(f"   üîí ‰∏çÂèØËÆ≠ÁªÉÂ±ÇÊï∞ (Frozen): {len(frozen_params)}")
    print(f"      - ÂèÇÊï∞Èáè: {frozen_numel / 1e9:.2f} B")
    print(f"   -------------------------------------------")
    
    # ÊâìÂç∞ÂÖ∑‰ΩìÂêçÂ≠óÔºà‰∏∫‰∫ÜÈò≤Ê≠¢Âà∑Â±èÔºåÊØèÁßçÂè™ÊâìÂç∞Ââç5‰∏™ÂíåÂêé5‰∏™Ôºâ
    if len(trainable_params) > 0:
        print(f"\nüìù ÂèØËÆ≠ÁªÉÂèÇÊï∞Á§∫‰æã (Top 5):")
        for p in trainable_params[:10]:
            print(f"   - [‚àö] {p}")
        if len(trainable_params) > 10: print("   ... (‰∏≠Èó¥ÁúÅÁï•) ...")
        # ÊâìÂç∞ÊúÄÂêéÂá†‰∏™ÔºåÁúãÁúãÈü≥È¢ëÈÉ®ÂàÜÂú®‰∏çÂú®
        for p in trainable_params[-10:]:
            print(f"   - [‚àö] {p}")
            
    if len(frozen_params) > 0:
        print(f"\nüßä ‰∏çÂèØËÆ≠ÁªÉÂèÇÊï∞Á§∫‰æã (Top 5):")
        for p in frozen_params[:10]:
            print(f"   - [x] {p}")
            
    print("="*50 + "\n")


def prepare_model_and_optimizer_groups(model, base_lr=1e-5, target_lr=1e-4):
    print("\n" + "="*50)
    print("üõ†Ô∏è  Ê≠£Âú®ÈÖçÁΩÆÊ®°ÂûãÂèÇÊï∞„ÄÅÂàùÂßãÂåñÂèäÂ≠¶‰π†ÁéáÂàÜÁªÑ...")
    print("="*50)

    # 1. ÂÆö‰πâÈ´òÂ≠¶‰π†ÁéáÔºà‰∏îÈúÄË¶ÅÁΩÆÈõ∂ÔºâÁöÑÁõÆÊ†áÊ®°ÂùóÂâçÁºÄ
    target_prefixes = (
        "audio_injector", 
        # "trainable_cond_mask", 
        # "frame_packer"
    )
    
    # 2. ÂÆπÂô®ÂàùÂßãÂåñ
    high_lr_params = []
    low_lr_params = []
    
    # ÁªüËÆ°Áî®ÂèòÈáè
    stats = {
        "high_lr_count": 0,    # È´òÂ≠¶‰π†ÁéáÂèÇÊï∞‰∏™Êï∞
        "low_lr_count": 0,     # ‰ΩéÂ≠¶‰π†ÁéáÂèÇÊï∞‰∏™Êï∞ (Backbone‰∏≠ÂéüÊú¨ÂèØËÆ≠ÁªÉÁöÑ)
        "frozen_skipped": 0,   # Ë¢´Ë∑≥ËøáÁöÑÂÜªÁªìÂèÇÊï∞ (Â¶Ç TextEncoder)
        "zero_value_count": 0, # ÂÆûÈôÖÂÄº‰∏∫0ÁöÑÂèÇÊï∞‰∏™Êï∞
        "total_params": 0
    }

    # 3. ÈÅçÂéÜÊ®°ÂûãÊâÄÊúâÂèÇÊï∞
    for name, param in model.named_parameters():
        stats["total_params"] += 1
        
        # Âà§Êñ≠ÊòØÂê¶Â±û‰∫éÁõÆÊ†áÊ®°Âùó (Audio/Mask/Packer)
        is_target_module = any(prefix in name for prefix in target_prefixes)
        
        if is_target_module:
            # ============================================
            # A. ÁõÆÊ†áÊ®°ÂùóÔºöÂº∫Âà∂ËÆ≠ÁªÉ + Âº∫Âà∂ÁΩÆÈõ∂ + È´òÂ≠¶‰π†Áéá
            # ============================================
            param.requires_grad = True # Á°Æ‰øùÂºÄÂêØ
            
            # ÊâßË°åÂÖ®ÈáèÁΩÆÈõ∂ (ÊÅ¢Â§ç‰Ω†‰πãÂâçÁöÑÈÄªËæë)
            # with torch.no_grad():
            #     param.zero_()
            
            high_lr_params.append(param)
            stats["high_lr_count"] += 1
            
            # È™åËØÅÁΩÆÈõ∂
            if param.sum() == 0:
                stats["zero_value_count"] += 1
                
        else:
            # ============================================
            # B. ÈùûÁõÆÊ†áÊ®°ÂùóÔºöÂ∞äÈáçÂéüÁä∂ÊÄÅ (Âè™Êî∂ÂΩïÊú¨Êù•Â∞±ÂºÄ‰∫ÜÊ¢ØÂ∫¶ÁöÑ)
            # ============================================
            if param.requires_grad:
                # ÂéüÊú¨Â∞±ÊòØÂèØËÆ≠ÁªÉÁöÑ (ÊØîÂ¶Ç Backbone ÁöÑ Attention) -> ‰ΩéÂ≠¶‰π†Áéá
                low_lr_params.append(param)
                stats["low_lr_count"] += 1
            else:
                # ÂéüÊú¨Â∞±ÊòØÂÜªÁªìÁöÑ (ÊØîÂ¶Ç Text Encoder) -> Ë∑≥ËøáÔºå‰∏çËøõ‰ºòÂåñÂô®
                stats["frozen_skipped"] += 1

    # 4. ÊâìÂç∞ËØ¶ÁªÜÁªüËÆ°Êä•Âëä
    print(f"\nüìä ÂèÇÊï∞ÁªüËÆ°Êä•Âëä:")
    print(f"   -------------------------------------------")
    print(f"   [Total] Ê®°ÂûãÊÄªÂèÇÊï∞Â±ÇÊï∞: {stats['total_params']}")
    print(f"   -------------------------------------------")
    print(f"   üî• [High LR Group] (Target Modules, lr={target_lr})")
    print(f"       - ÂåÖÂê´: {target_prefixes}")
    print(f"       - Êï∞Èáè: {stats['high_lr_count']}")
    print(f"       - ÁΩÆÈõ∂È™åËØÅ: {stats['zero_value_count']} / {stats['high_lr_count']} (Â∫îÁõ∏Á≠â)")
    
    print(f"   ‚ùÑÔ∏è [Low LR Group] (Backbone SFT, lr={base_lr})")
    print(f"       - Êï∞Èáè: {stats['low_lr_count']}")
    print(f"       - ËØ¥Êòé: Ëøô‰∫õÊòØSFTÊùÉÈáç‰∏≠ÂéüÊú¨ÂºÄÂêØÊ¢ØÂ∫¶ÁöÑÈÉ®ÂàÜ")
    
    print(f"   üßä [Skipped/Frozen] (Not Training)")
    print(f"       - Êï∞Èáè: {stats['frozen_skipped']}")
    print(f"       - ËØ¥Êòé: Ëøô‰∫õÂèÇÊï∞‰øùÊåÅÂÜªÁªìÔºå‰∏çÊ∂àËÄóÊòæÂ≠òÂ≠òÊ¢ØÂ∫¶ (Â¶ÇTextEncoder)")
    print(f"   -------------------------------------------")

    # 5. ÊûÑÂª∫‰ºòÂåñÂô®ÊâÄÈúÄÁöÑÂèÇÊï∞ÁªÑÂàóË°®
    optimizer_grouped_parameters = [
        {
            "params": low_lr_params, 
            "lr": base_lr,
            "name": "backbone_low_lr"
        },
        {
            "params": high_lr_params, 
            "lr": target_lr,
            "name": "audio_new_high_lr"
        }
    ]
    
    return optimizer_grouped_parameters

def launch_training_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    learning_rate: float = 1e-5,
    weight_decay: float = 1e-2,
    num_workers: int = 1,
    save_steps: int = None,
    num_epochs: int = 1,
    args = None,
):
    if args is not None:
        # small_lr_rate = 1e-5
        learning_rate = args.learning_rate
        weight_decay = args.weight_decay
        num_workers = args.dataset_num_workers
        save_steps = args.save_steps
        num_epochs = args.num_epochs
        debug = args.debug
    
    if debug:
        diagnose_default_training_status(model)
    # optimizer = torch.optim.AdamW(model.trainable_modules(), lr=learning_rate, weight_decay=weight_decay)
    optimizer_grouped_parameters = prepare_model_and_optimizer_groups(
        model, 
        base_lr=1e-5, 
        target_lr=learning_rate
    )
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers) if debug else torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=lambda x: x[0], num_workers=num_workers)
    
    model, optimizer, dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)
    
    if debug:
        model_logger.on_training_start(accelerator, model)
        timer = StepTimer()
        end_time = time.perf_counter()
        for epoch_id in range(num_epochs):
            for step_index, data in enumerate(tqdm(dataloader, desc=f"Epoch {epoch_id}")):
                
                data_load_time = time.perf_counter() - end_time
                timer.record("data_loading", data_load_time)
                
                if step_index > 10:
                    break
                    
                with accelerator.accumulate(model):
                    
                    with timer.time_step("zero_grad"):
                        optimizer.zero_grad()
                    
                    with timer.time_step("forward"):
                        loss = model(data)
                    
                    with timer.time_step("backward"):
                        accelerator.backward(loss)
                    
                    with timer.time_step("optimizer.step"):
                        optimizer.step()
                    
                    with timer.time_step("model_logger"):
                        model_logger.on_step_end(accelerator, model, save_steps)
                    
                    with timer.time_step("scheduler.step"):
                        scheduler.step()

                end_time = time.perf_counter()

        timer.print_summary()
        model_logger.on_training_end(accelerator, model, save_steps)

    else:
        model_logger.on_training_start(accelerator, model)
    
        for epoch_id in range(num_epochs):
            for data in tqdm(dataloader):
                with accelerator.accumulate(model):
                    optimizer.zero_grad()
                    loss = model(data)
                    accelerator.backward(loss)
                    optimizer.step()
                    model_logger.on_step_end(accelerator, model, save_steps)
                    scheduler.step() # Ëøô‰∏ÄÊ≠•‰∏∫‰ªÄ‰πàËøô‰πàÊÖ¢
        model_logger.on_training_end(accelerator, model, save_steps)


def launch_data_process_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    num_workers: int = 8,
    args = None,
):
    if args is not None:
        num_workers = args.dataset_num_workers
        
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers)
    model, dataloader = accelerator.prepare(model, dataloader)
    
    for data_id, data in enumerate(tqdm(dataloader)):
        with accelerator.accumulate(model):
            with torch.no_grad():
                folder = os.path.join(model_logger.output_path, str(accelerator.process_index))
                os.makedirs(folder, exist_ok=True)
                save_path = os.path.join(model_logger.output_path, str(accelerator.process_index), f"{data_id}.pth")
                data = model(data)
                torch.save(data, save_path)
