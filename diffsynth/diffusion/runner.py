import os, torch
from tqdm import tqdm
from accelerate import Accelerator
from .training_module import DiffusionTrainingModule
from .logger import ModelLogger
import torch
from tqdm import tqdm
import time
import time
from contextlib import contextmanager
from collections import defaultdict

import time
from collections import defaultdict
from contextlib import contextmanager
import statistics


import os
import torch
from datetime import datetime

class StepTimer:
    def __init__(self, log_file="training_perf.log"):
        self.times = defaultdict(list)
        self.step_keys = []
        self.log_file = log_file

    @contextmanager
    def time_step(self, name):
        if name not in self.step_keys:
            self.step_keys.append(name)
        # ç¡®ä¿ GPU åŒæ­¥ï¼Œå¦åˆ™æ—¶é—´ç»Ÿè®¡ä¸å‡†
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        start = time.perf_counter()
        yield
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        self.times[name].append(elapsed)

    def record(self, name, elapsed):
        if name not in self.step_keys:
            self.step_keys.append(name)
        self.times[name].append(elapsed)

    def print_summary(self, accelerator):
        # ä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œæ‰“å°å’Œå†™æ–‡ä»¶
        if not accelerator.is_main_process:
            return

        if not self.step_keys:
            return
            
        # ä»¥è®°å½•æœ€å¤šçš„ key ä¸ºå‡†ï¼ˆé€šå¸¸æ˜¯ data_loadingï¼‰
        num_steps = max(len(self.times[k]) for k in self.step_keys)
        
        output = []
        output.append("\n" + "="*120)
        output.append(f"{'Step è€—æ—¶è¯¦æƒ… (å•ä½: ms) - ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'):^120s}")
        output.append("="*120)
        
        headers = ["Step"] + self.step_keys + ["Total"]
        col_width = 15 
        header_str = "".join([f"{h:>{col_width}s}" for h in headers])
        output.append(header_str)
        output.append("-" * len(header_str))

        for i in range(num_steps):
            row_vals = []
            step_total = 0.0
            for key in self.step_keys:
                # å¥å£®æ€§å¤„ç†ï¼šå¦‚æœæŸé¡¹æ²¡æœ‰è®°å½•ï¼ˆæ¯”å¦‚æ¢¯åº¦ç´¯ç§¯è·³è¿‡äº†ï¼‰ï¼Œè®°ä¸º 0
                if i < len(self.times[key]):
                    val = self.times[key][i] * 1000
                else:
                    val = 0.0
                step_total += val
                row_vals.append(f"{val:{col_width}.2f}")
            
            row_str = f"{i:>{col_width}d}" + "".join(row_vals) + f"{step_total:{col_width}.2f}"
            output.append(row_str)

        # ç»Ÿè®¡æ‘˜è¦
        output.append("\n" + "="*120)
        output.append(f"{'ç»Ÿè®¡æ‘˜è¦ (å¹³å‡å€¼)':^120s}")
        output.append("="*120)
        total_time_all_steps = sum(sum(v) for v in self.times.values())
        
        for name in self.step_keys:
            values = self.times[name]
            if values:
                avg = statistics.mean(values) * 1000
                total = sum(values)
                ratio = (total / total_time_all_steps) * 100 if total_time_all_steps > 0 else 0
                output.append(f"{name:<25s} | å¹³å‡: {avg:10.2f} ms | æ€»è®¡: {total:10.2f} s | å æ¯”: {ratio:7.1f}%")
        
        final_log = "\n".join(output)
        
        # 1. æ‰“å°åˆ°ç»ˆç«¯
        print(final_log)
        
        # 2. ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(final_log + "\n")

def diagnose_default_training_status(model):
    """
    è¯Šæ–­æ¨¡å‹å½“å‰çš„é»˜è®¤è®­ç»ƒçŠ¶æ€ï¼ˆåœ¨äººå·¥ä¿®æ”¹ requires_grad ä¹‹å‰ï¼‰
    """
    print("\n" + "="*50)
    print("ğŸ•µï¸ [è¯Šæ–­æ¨¡å¼] æ£€æŸ¥æ¨¡å‹é»˜è®¤è®­ç»ƒçŠ¶æ€...")
    print("="*50)
    
    trainable_params = []
    frozen_params = []
    
    trainable_numel = 0
    frozen_numel = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            trainable_params.append(name)
            trainable_numel += param.numel()
        else:
            frozen_params.append(name)
            frozen_numel += param.numel()
            
    # ç»Ÿè®¡æ•°æ®
    total_layers = len(trainable_params) + len(frozen_params)
    total_params = trainable_numel + frozen_numel
    
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - æ€»å±‚æ•° (Keys): {total_layers}")
    print(f"   - æ€»å‚æ•°é‡ (Elements): {total_params / 1e9:.2f} B (åäº¿)")
    print(f"   -------------------------------------------")
    print(f"   ğŸ”“ å¯è®­ç»ƒå±‚æ•° (Trainable): {len(trainable_params)}")
    print(f"      - å‚æ•°é‡: {trainable_numel / 1e9:.2f} B")
    print(f"      - å æ¯”: {trainable_numel / total_params * 100:.2f}%")
    print(f"   ğŸ”’ ä¸å¯è®­ç»ƒå±‚æ•° (Frozen): {len(frozen_params)}")
    print(f"      - å‚æ•°é‡: {frozen_numel / 1e9:.2f} B")
    print(f"   -------------------------------------------")
    
    # æ‰“å°å…·ä½“åå­—ï¼ˆä¸ºäº†é˜²æ­¢åˆ·å±ï¼Œæ¯ç§åªæ‰“å°å‰5ä¸ªå’Œå5ä¸ªï¼‰
    if len(trainable_params) > 0:
        print(f"\nğŸ“ å¯è®­ç»ƒå‚æ•°ç¤ºä¾‹ (Top 5):")
        for p in trainable_params[:10]:
            print(f"   - [âˆš] {p}")
        if len(trainable_params) > 10: print("   ... (ä¸­é—´çœç•¥) ...")
        # æ‰“å°æœ€åå‡ ä¸ªï¼Œçœ‹çœ‹éŸ³é¢‘éƒ¨åˆ†åœ¨ä¸åœ¨
        for p in trainable_params[-10:]:
            print(f"   - [âˆš] {p}")
            
    if len(frozen_params) > 0:
        print(f"\nğŸ§Š ä¸å¯è®­ç»ƒå‚æ•°ç¤ºä¾‹ (Top 5):")
        for p in frozen_params[:10]:
            print(f"   - [x] {p}")
            
    print("="*50 + "\n")


def prepare_model_and_optimizer_groups(model, base_lr=1e-5, target_lr=1e-4):
    print("\n" + "="*50)
    print("ğŸ› ï¸  æ­£åœ¨é…ç½®æ¨¡å‹å‚æ•°ã€åˆå§‹åŒ–åŠå­¦ä¹ ç‡åˆ†ç»„...")
    print("="*50)

    # 1. å®šä¹‰é«˜å­¦ä¹ ç‡ï¼ˆä¸”éœ€è¦ç½®é›¶ï¼‰çš„ç›®æ ‡æ¨¡å—å‰ç¼€
    target_prefixes = (
        "audio_injector", 
        # "trainable_cond_mask", 
        # "frame_packer"
    )
    
    # 2. å®¹å™¨åˆå§‹åŒ–
    high_lr_params = []
    low_lr_params = []
    
    # ç»Ÿè®¡ç”¨å˜é‡
    stats = {
        "high_lr_count": 0,    # é«˜å­¦ä¹ ç‡å‚æ•°ä¸ªæ•°
        "low_lr_count": 0,     # ä½å­¦ä¹ ç‡å‚æ•°ä¸ªæ•° (Backboneä¸­åŸæœ¬å¯è®­ç»ƒçš„)
        "frozen_skipped": 0,   # è¢«è·³è¿‡çš„å†»ç»“å‚æ•° (å¦‚ TextEncoder)
        "zero_value_count": 0, # å®é™…å€¼ä¸º0çš„å‚æ•°ä¸ªæ•°
        "total_params": 0
    }

    # 3. éå†æ¨¡å‹æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        stats["total_params"] += 1
        
        # åˆ¤æ–­æ˜¯å¦å±äºç›®æ ‡æ¨¡å— (Audio/Mask/Packer)
        is_target_module = any(prefix in name for prefix in target_prefixes)
        
        if is_target_module:
            # ============================================
            # A. ç›®æ ‡æ¨¡å—ï¼šå¼ºåˆ¶è®­ç»ƒ + å¼ºåˆ¶ç½®é›¶ + é«˜å­¦ä¹ ç‡
            # ============================================
            param.requires_grad = True # ç¡®ä¿å¼€å¯
            
            # æ‰§è¡Œå…¨é‡ç½®é›¶ (æ¢å¤ä½ ä¹‹å‰çš„é€»è¾‘)
            # with torch.no_grad():
            #     param.zero_()
            
            high_lr_params.append(param)
            stats["high_lr_count"] += 1
            
            # éªŒè¯ç½®é›¶
            if param.sum() == 0:
                stats["zero_value_count"] += 1
                
        else:
            # ============================================
            # B. éç›®æ ‡æ¨¡å—ï¼šå°Šé‡åŸçŠ¶æ€ (åªæ”¶å½•æœ¬æ¥å°±å¼€äº†æ¢¯åº¦çš„)
            # ============================================
            if param.requires_grad:
                # åŸæœ¬å°±æ˜¯å¯è®­ç»ƒçš„ (æ¯”å¦‚ Backbone çš„ Attention) -> ä½å­¦ä¹ ç‡
                low_lr_params.append(param)
                stats["low_lr_count"] += 1
            else:
                # åŸæœ¬å°±æ˜¯å†»ç»“çš„ (æ¯”å¦‚ Text Encoder) -> è·³è¿‡ï¼Œä¸è¿›ä¼˜åŒ–å™¨
                stats["frozen_skipped"] += 1

    # 4. æ‰“å°è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡æŠ¥å‘Š:")
    print(f"   -------------------------------------------")
    print(f"   [Total] æ¨¡å‹æ€»å‚æ•°å±‚æ•°: {stats['total_params']}")
    print(f"   -------------------------------------------")
    print(f"   ğŸ”¥ [High LR Group] (Target Modules, lr={target_lr})")
    print(f"       - åŒ…å«: {target_prefixes}")
    print(f"       - æ•°é‡: {stats['high_lr_count']}")
    print(f"       - ç½®é›¶éªŒè¯: {stats['zero_value_count']} / {stats['high_lr_count']} (åº”ç›¸ç­‰)")
    
    print(f"   â„ï¸ [Low LR Group] (Backbone SFT, lr={base_lr})")
    print(f"       - æ•°é‡: {stats['low_lr_count']}")
    print(f"       - è¯´æ˜: è¿™äº›æ˜¯SFTæƒé‡ä¸­åŸæœ¬å¼€å¯æ¢¯åº¦çš„éƒ¨åˆ†")
    
    print(f"   ğŸ§Š [Skipped/Frozen] (Not Training)")
    print(f"       - æ•°é‡: {stats['frozen_skipped']}")
    print(f"       - è¯´æ˜: è¿™äº›å‚æ•°ä¿æŒå†»ç»“ï¼Œä¸æ¶ˆè€—æ˜¾å­˜å­˜æ¢¯åº¦ (å¦‚TextEncoder)")
    print(f"   -------------------------------------------")

    # 5. æ„å»ºä¼˜åŒ–å™¨æ‰€éœ€çš„å‚æ•°ç»„åˆ—è¡¨
    optimizer_grouped_parameters = [
        {
            "params": low_lr_params, 
            "lr": base_lr,
            "name": "backbone_low_lr"
        },
        {
            "params": high_lr_params, 
            "lr": target_lr,
            "name": "audio_new_high_lr"
        }
    ]
    
    return optimizer_grouped_parameters

def launch_training_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    learning_rate: float = 1e-5,
    weight_decay: float = 1e-2,
    num_workers: int = 1,
    save_steps: int = None,
    num_epochs: int = 1,
    args = None,
):
    if args is not None:
        # small_lr_rate = 1e-5
        learning_rate = args.learning_rate
        weight_decay = args.weight_decay
        num_workers = args.dataset_num_workers
        save_steps = args.save_steps
        num_epochs = args.num_epochs
        debug = args.debug
    
    if debug:
        diagnose_default_training_status(model)
    # optimizer = torch.optim.AdamW(model.trainable_modules(), lr=learning_rate, weight_decay=weight_decay)
    optimizer_grouped_parameters = prepare_model_and_optimizer_groups(
        model, 
        base_lr=1e-5, 
        target_lr=learning_rate
    )
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers) if debug else torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=lambda x: x[0], num_workers=num_workers)
    
    model, optimizer, dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)
    
    if debug:
        model_logger.on_training_start(accelerator, model)
        log_name = f"perf_debug_{datetime.now().strftime('%m%d_%H%M')}.log"
        timer = StepTimer(log_file=log_name)
        for epoch_id in range(num_epochs):
            end_time = time.perf_counter()
            for step_index, data in enumerate(tqdm(dataloader, desc=f"Epoch {epoch_id}", disable=not accelerator.is_main_process)):
                
                # 1. æå‰é€€å‡ºåˆ¤æ–­
                if step_index > 50:
                    break
                
                # 2. è®°å½•æ•°æ®åŠ è½½æ—¶é—´
                data_load_time = time.perf_counter() - end_time
                timer.record("data_loading", data_load_time)
                
                with accelerator.accumulate(model):
                    
                    with timer.time_step("zero_grad"):
                        optimizer.zero_grad()
                    
                    with timer.time_step("forward"):
                        loss = model(data)
                    
                    with timer.time_step("backward"):
                        accelerator.backward(loss)
                    
                    if accelerator.sync_gradients:
                        with timer.time_step("optimizer.step"):
                            optimizer.step()
                        
                        with timer.time_step("model_logger"):
                            model_logger.on_step_end(accelerator, model, save_steps)
                        
                        with timer.time_step("scheduler.step"):
                            scheduler.step()
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰§è¡Œæ›´æ–°ï¼Œå¡«å…… 0 ä»¥ä¿æŒ Timer å†…éƒ¨åˆ—è¡¨å¯¹é½
                        timer.record("optimizer.step", 0)
                        timer.record("model_logger", 0)
                        timer.record("scheduler.step", 0)

                # é‡ç½® end_time ç”¨äºä¸‹ä¸€è½® data_loading ç»Ÿè®¡
                end_time = time.perf_counter()

        # æ‰“å°å¹¶ä¿å­˜ç»“æœ
        accelerator.wait_for_everyone() # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        timer.print_summary(accelerator)
        model_logger.on_training_end(accelerator, model, save_steps)

    else:
        model_logger.on_training_start(accelerator, model)
    
        for epoch_id in range(num_epochs):
            for data in tqdm(dataloader):
                with accelerator.accumulate(model):
                    optimizer.zero_grad()
                    loss = model(data)
                    accelerator.backward(loss)
                    optimizer.step()
                    model_logger.on_step_end(accelerator, model, save_steps)
                    scheduler.step() # è¿™ä¸€æ­¥ä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢
        model_logger.on_training_end(accelerator, model, save_steps)


def launch_data_process_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    num_workers: int = 8,
    args = None,
):
    if args is not None:
        num_workers = args.dataset_num_workers
        
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers)
    model, dataloader = accelerator.prepare(model, dataloader)
    
    for data_id, data in enumerate(tqdm(dataloader)):
        with accelerator.accumulate(model):
            with torch.no_grad():
                folder = os.path.join(model_logger.output_path, str(accelerator.process_index))
                os.makedirs(folder, exist_ok=True)
                save_path = os.path.join(model_logger.output_path, str(accelerator.process_index), f"{data_id}.pth")
                data = model(data)
                torch.save(data, save_path)
